{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CYBER: A General Robotic Operation System for Embodied AI","text":"<p>The development of world models in robotics has long been a cornerstone of advanced research, with most approaches relying heavily on vast, platform-specific datasets. These datasets, while valuable, often limit scalability and generalization to different robotic platforms, restricting their broader applicability.</p> <p>In contrast, CYBER approaches world modeling from a \"first principles\" perspective, drawing inspiration from how humans naturally acquire skills through experience and interaction with their environment. CYBER is the first general Robotic Operational System designed to adapt to both teleoperated manipulation and human operation data, enabling robots to learn and predict across a wide range of tasks and environments. It builds with a Physical World Model, a cross-embodied Visual-Language Action Model (VLA), a Perception Model, a Memory Model, and a Control Model to help robots learn, predict, and memory across various tasks and embodiments.</p> <p>At the same time, CYBER also provide millions of human operation datasets and baseline models over HuggingFace \ud83e\udd17 to enhance embodied learning, and experimental evalaution tool box to help researchers to test and evaluate their models in both simulation and real world.</p>"},{"location":"#modular-components","title":"\ud83d\udee0\ufe0f Modular Components","text":"<p>CYBER is built with a modular architecture, allowing for flexibility and customization. Here are the key components:</p> <ul> <li>\ud83c\udf0d World Model: Learns from physical interactions to understand and predict the environment.</li> <li>\ud83c\udfac Action Model: Learns from actions and interactions to perform tasks and navigate.</li> <li>\ud83d\udc41\ufe0f Perception Model: Processes sensory inputs to perceive and interpret surroundings.</li> <li>\ud83e\udde0 Memory Model: Utilizes past experiences to inform current decisions.</li> <li>\ud83c\udfae Control Model: Manages control inputs for movement and interaction.</li> </ul>"},{"location":"#release","title":"\ud83d\udcf0 Release","text":"<ul> <li>2024-11-18: \ud83c\udf0d World Model supports new tokenizer model Cosmos-Tokenizer and new dynamic model Deep Planning Network</li> <li>2024-10-23: \ud83c\udf0d World Model is now available. Additional models will be released soon.</li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>If you have technical questions, please open a GitHub issue. For business development or other collaboration inquiries, feel free to contact us through email \ud83d\udce7 (contact@cyberorigin.ai). Enjoy! \ud83c\udf89</p>"},{"location":"features/","title":"Key Features","text":"<ul> <li>\ud83d\udee0\ufe0f Modular: Built with a modular architecture, allowing flexibility in various environments.</li> <li>\ud83d\udcca Data-Driven: Leverages millions of human operation datasets to enhance embodied learning.</li> <li>\ud83d\udcc8 Scalable: Scales across different robotic platforms, adapting to new environments and tasks.</li> <li>\ud83d\udd27 Customizable: Allows for customization and fine-tuning to meet specific requirements.</li> <li>\ud83d\udcda Extensible: Supports the addition of new modules and functionalities, enhancing capabilities.</li> <li>\ud83d\udce6 Open Source: Open-source and freely available, fostering collaboration and innovation.</li> <li>\ud83d\udd2c Experimental: Supports experimentation and testing, enabling continuous improvement.</li> </ul>"},{"location":"file_structure/","title":"File Structure","text":"<pre><code>\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 docs                   # documentation files and figures \n\u251c\u2500\u2500 docker                 # docker files for containerization\n\u251c\u2500\u2500 examples               # example code snippets\n\u251c\u2500\u2500 tests                  # test cases and scripts\n\u251c\u2500\u2500 scripts                # scripts for setup and utilities\n\u251c\u2500\u2500 experiments            # model implementation and details\n\u2502   \u251c\u2500\u2500 configs            # model configurations\n\u2502   \u251c\u2500\u2500 models             # model training and evaluation scripts\n\u2502   \u251c\u2500\u2500 notebooks          # sample notebooks\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 cyber                  # compression, model training, and dataset source code\n\u2502   \u251c\u2500\u2500 dataset            # dataset processing and loading\n\u2502   \u251c\u2500\u2500 utils              # utility functions\n\u2502   \u2514\u2500\u2500 models             # model definitions and architectures\n\u2502       \u251c\u2500\u2500 action         # visual language action model\n\u2502       \u251c\u2500\u2500 control        # robot platform control model\n\u2502       \u251c\u2500\u2500 memory         # lifelong memory model\n\u2502       \u251c\u2500\u2500 perception     # perception and scene understanding model\n\u2502       \u251c\u2500\u2500 world          # physical world model\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"huggingface/","title":"Hugging Face Integration","text":"<p>CYBER leverages the power of Hugging Face for model sharing and collaboration. You can easily access and use our models through the Hugging Face platform.</p>"},{"location":"huggingface/#available-data","title":"Available Data","text":"<p>Currently, four tasks are available for download:</p> <ul> <li>\ud83e\udd17 Pipette: Bimanual human demonstration dataset of precision pipetting tasks for laboratory manipulation.</li> <li>\ud83e\udd17 Take Item: Single-arm manipulation demonstrations of object pick-and-place tasks.</li> <li>\ud83e\udd17 Twist Tube: Bimanual demonstration dataset of coordinated tube manipulation sequences.</li> <li>\ud83e\udd17 Fold Towels: Bimanual manipulation demonstrations of deformable object folding procedures.</li> </ul>"},{"location":"huggingface/#available-models","title":"Available Models","text":"<p>Our pretrained models will be released on Hugging Face soon:</p> <ul> <li>Cyber-World-Large (Coming Soon)</li> <li> <p>Cyber-World-Base</p> </li> <li> <p>Cyber-World-Small (Coming Soon)</p> </li> </ul>"},{"location":"reference/","title":"References","text":"<p>We could like to acknowledge the following projects where parts of codes adapted from: * Open-MAGVIT2 * 1xGPT Challenge * Cosmos-Tokenizer * Deep Planning Network * [Pytorch-Planet] (https://github.com/abhayraw1/planet-torch)</p> <p><pre><code>@inproceedings{wang2024hpt,\nauthor    = {Lirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He},\ntitle     = {Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers},\nbooktitle = {Neurips},\nyear      = {2024}\n}\n</code></pre> <pre><code>@article{luo2024open,\n  title={Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation},\n  author={Luo, Zhuoyan and Shi, Fengyuan and Ge, Yixiao and Yang, Yujiu and Wang, Limin and Shan, Ying},\n  journal={arXiv preprint arXiv:2409.04410},\n  year={2024}\n}\n</code></pre> <pre><code>@inproceedings{hafner2019learning,\n  title={Learning latent dynamics for planning from pixels},\n  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},\n  booktitle={International conference on machine learning},\n  pages={2555--2565},\n  year={2019},\n  organization={PMLR}\n}\n</code></pre> <pre><code>@inproceedings{pmlr-v235-bruce24a,\n  title =    {Genie: Generative Interactive Environments},\n  author =       {Bruce, Jake and Dennis, Michael D and Edwards, Ashley and Parker-Holder, Jack and Shi, Yuge and Hughes, Edward and Lai, Matthew and Mavalankar, Aditi and Steigerwald, Richie and Apps, Chris and Aytar, Yusuf and Bechtle, Sarah Maria Elisabeth and Behbahani, Feryal and Chan, Stephanie C.Y. and Heess, Nicolas and Gonzalez, Lucy and Osindero, Simon and Ozair, Sherjil and Reed, Scott and Zhang, Jingwei and Zolna, Konrad and Clune, Jeff and Freitas, Nando De and Singh, Satinder and Rockt\\\"{a}schel, Tim},\n  booktitle =    {Proceedings of the 41st International Conference on Machine Learning},\n  pages =    {4603--4623},\n  year =     {2024},\n  editor =   {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},\n  volume =   {235},\n  series =   {Proceedings of Machine Learning Research},\n  month =    {21--27 Jul},\n  publisher =    {PMLR},\n  pdf =      {https://raw.githubusercontent.com/mlresearch/v235/main/assets/bruce24a/bruce24a.pdf},\n  url =      {https://proceedings.mlr.press/v235/bruce24a.html},\n  abstract =     {We introduce Genie, the first &lt;em&gt;generative interactive environment&lt;/em&gt; trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a &lt;em&gt;foundation world model&lt;/em&gt;. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis &lt;em&gt;despite training without any ground-truth action labels&lt;/em&gt; or other domain specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.}\n}\n</code></pre></p>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#installation","title":"Installation","text":"<p>You can run the following commands to install CYBER:</p> <pre><code>bash scripts/build.sh\n</code></pre> <p>Alternatively, you can install it manually by following the steps below:</p> <ol> <li> <p>Create a clean conda environment:</p> <pre><code>conda create -n cyber python=3.10 &amp;&amp; conda activate cyber\n</code></pre> </li> <li> <p>Install PyTorch and torchvision:</p> <pre><code>conda install pytorch==2.3.0 torchvision==0.18.0 cudatoolkit=11.1 -c pytorch -c nvidia\n</code></pre> </li> <li> <p>Install the CYBER package:</p> <pre><code>pip install -e .\n</code></pre> </li> </ol>"},{"location":"experiments/world_model/","title":"Cyber","text":"<p>Cyber represents a model implementation that seamlessly integrates state-of-the-art (SOTA) world models with the proposed CyberOrigin Dataset, pushing the boundaries of artificial intelligence and machine learning.</p> <p>Follow this document to train the models using our readily-available data or adapt your data for training.</p>"},{"location":"experiments/world_model/#cyberorigin-dataset","title":"CyberOrigin Dataset","text":"<p>Our data includes information from home services, the logistics industry, and laboratory scenarios. For more details, please refer to our Offical Data Website.</p> <ul> <li> <p>Format &amp; Description Currently, the dataset contains image tokens generated by Magvit2. For more information, please refer to the dataset card on Huggingface.</p> </li> <li> <p>Download the Dataset The dataset is currently available on Huggingface.</p> <ul> <li>\ud83e\udd17 Pipette</li> <li>\ud83e\udd17 Take Item</li> <li>\ud83e\udd17 Twist Tube</li> <li>\ud83e\udd17 Fold Towels</li> </ul> </li> </ul> <p>You can download the dataset using the following command: <pre><code>bash ../scripts/download_dataset.sh\n</code></pre></p> <ul> <li>Visualize the Dataset You can visualize the dataset using this notebook. Make sure to install the jupyter before running the notebook. <code>pip install jupyter notebook</code></li> </ul>"},{"location":"experiments/world_model/#quick-start-for-cyberorigin-dataset","title":"Quick Start for CyberOrigin Dataset","text":""},{"location":"experiments/world_model/#download-the-dataset","title":"Download the Dataset","text":"<pre><code>bash ../scripts/download_dataset.sh\n</code></pre>"},{"location":"experiments/world_model/#model-training-evaluation","title":"Model Training &amp; Evaluation","text":"<p>The following steps will guide you through training the a GENIE dynamic model on the CyberOrigin dataset.</p> <p>Local Training To train on your local machine using a single GPU, run: <pre><code>python models/world/train_dynamic.py --data_dir data/cyber_pipette/data\n</code></pre> Note: The model will train on the default configuration provided.</p> <p>Training on Sagemaker Using AWS Sagemaker for training allows you to leverage multiple GPUs on the cloud to speed up training. To train on Sagemaker, follow the instructions in the Sagemaker README.</p>"},{"location":"experiments/world_model/#model-configuration-and-hyperparameters","title":"Model configuration and hyperparameters","text":""},{"location":"experiments/world_model/#genie","title":"GENIE","text":"<p>The code is adapted from 1x's implementation of GENIE. The model is based on an ST-transformer architecture that predicts the next frame given the previous frames.</p> <p>Model parameters tuning The detailed configuration file is provided in the <code>configs/models/world</code> folder.  <pre><code>{\n    \"num_layers\": 32, // number of ST-transformer blocks\n    \"num_heads\": 8, // number of heads in multi-head attention\n    \"d_model\": 256, // dimension of the model latent\n    \"T\": 16, // number of frames in the input sequence\n    \"S\": 256, // number of tokens in the input sequence S=16x16\n    \"image_vocab_size\": 262144, // codebook size for the image tokens\n    \"use_mup\": false, // whether to use MUP\n    \"num_factored_vocabs\": 2, // number of factored vocabularies\n    \"qkv_bias\": false, // whether to use bias in qkv projection\n    \"proj_bias\": true, // whether to use bias in projection\n    \"attn_drop\": 0, // dropout rate in attention\n    \"qk_norm\": false, // whether to normalize qk\n    \"mlp_ratio\": 4, // ratio of hidden size to model latent size in MLP\n    \"mlp_drop\": 0, // dropout rate in MLP\n    \"mlp_bias\": true // whether to use bias in MLP\n}\n</code></pre> It is recommended to only modify the first three parameters to adjust model size.</p> <p>Training parameters tuning Please refer to the help message for hyperparameter descriptions <pre><code>python models/world/train.py -h\n</code></pre></p>"},{"location":"experiments/world_model/#open-magvit2","title":"Open-MAGVIT2","text":"<p>The code is modified from 1XGPT and Open-MAGVIT2 but removed unnecessary files and code.</p> <p>Pretrained checkpoint Download the checkpoint HERE Or run the command:</p> <pre><code>huggingface-cli download TencentARC/Open-MAGVIT2 imagenet_256_L.ckpt --repo-type dataset --local-dir ./experiments/\n</code></pre> <p>Try with our provided samples We provide the notebook you can try to compress and decompress your video. Please try autoencoder_demo.ipynb and follow the instructions.</p> <p>Compress your video data Please follow the command below to encode and decode your data.</p> <p>Compress videos to tokens: <pre><code>python experiments/notebooks/compress_and_recon.py --config_file experiments/configs/models/world/openmagvit2.yaml --ckpt_path path/to/ckpt/file --video_path path/to/video/file --save_dir path/to/output/file --mode encode\n</code></pre></p> <p>Reconstruct videos from tokens: <pre><code>python experiments/notebooks/compress_and_recon.py --config_file experiments/configs/models/world/openmagvit2.yaml --ckpt_path path/to/ckpt/file --tokens_path path/to/tokens/file --save_dir path/to/output/file --mode decode\n</code></pre></p> <p>Model training <pre><code>image-folder\n    \u251c\u2500\u2500 image_1.png\n    \u251c\u2500\u2500 image_2.png\n    \u251c\u2500\u2500 image_3.png\n    \u251c\u2500\u2500 image_4.png\n    \u251c\u2500\u2500 ...\n</code></pre></p> <p>The following command instructs you to train the Open-Magvit2 tokenizer on your customized image dataset, please ensure your data is the same structure as above. <pre><code>python experiments/models/world/train_openmagvit2.py --config experiments/configs/models/world/openmagvit2.yaml --data_dir path/to/image/folder --output_dir path/to/output/folder\n</code></pre></p> <p>Please refer to openmagvit2.yaml for more hyperparameter descriptions.</p>"},{"location":"experiments/world_model/#cosmos-tokenizer","title":"Cosmos-Tokenizer","text":"<p>The code is modified from Cosmos-Tokenizer but removed unnecessary files and code. Currently, Cosmos-Tokenizer is available for inference only.</p> <p>Pretrained checkpoint Download the checkpoint HERE Or follow this snippet below:</p> <p><pre><code>from huggingface_hub import login, snapshot_download\nimport os\n\nlogin(token=\"&lt;YOUR-HF-TOKEN&gt;\", add_to_git_credential=True)\nmodel_names = [\n        \"Cosmos-Tokenizer-CI8x8\",\n        \"Cosmos-Tokenizer-CI16x16\",\n        \"Cosmos-Tokenizer-CV4x8x8\",\n        \"Cosmos-Tokenizer-CV8x8x8\",\n        \"Cosmos-Tokenizer-CV8x16x16\",\n        \"Cosmos-Tokenizer-DI8x8\",\n        \"Cosmos-Tokenizer-DI16x16\",\n        \"Cosmos-Tokenizer-DV4x8x8\",\n        \"Cosmos-Tokenizer-DV8x8x8\",\n        \"Cosmos-Tokenizer-DV8x16x16\",\n]\nfor model_name in model_names:\n    hf_repo = \"nvidia/\" + model_name\n    local_dir = \"pretrained_ckpts/\" + model_name\n    os.makedirs(local_dir, exist_ok=True)\n    print(f\"downloading {model_name}...\")\n    snapshot_download(repo_id=hf_repo, local_dir=local_dir)\n</code></pre> Try with our provided samples We provide the notebook you can try to encode your data in discrete tokens and continuous latent space, and decode tokens for visualization. Please try cosmos_demo.ipynb and follow the instructions.</p>"},{"location":"experiments/sagemaker/","title":"Training Using Sagemaker","text":"<p>This folder contains the additional config files and scripts to train the GENIE model on AWS Sagemaker.</p> <ul> <li>accelerate_config.yaml:  Accelerate configuration file for distributed training.</li> <li>entry.py: Entry point for the training job.</li> <li>README.md: Instructions for setting up the training job.</li> <li>run.ipynb: Jupyter notebook for queueing the training job.</li> <li>train_script_sagemaker.sh: Script to invoke the training script using accelerate.</li> </ul>"},{"location":"experiments/sagemaker/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account with Sagemaker. You can follow the official guide if you want to know more about SM training jobs and verify your account settings.</li> <li>WandB account for logging. You can create an account here.</li> <li>Credits. (1 hour of training on p4.xlarge instance costs around $30 :D)</li> </ul>"},{"location":"experiments/sagemaker/#training","title":"Training","text":"<p>1. Open a notebook instance on sagemaker. You can follow the official guide to create a notebook instance.</p> <p>2. Create a terminal in the notebook instance. Run the following commands to create a working directory and clone this repo. <pre><code>mkdir training\ncd training\ngit clone https://github.com/CyberOrigin2077/Cyber.git\n</code></pre> After this step your folder structure should look like this: <pre><code>training\n\u2514\u2500\u2500 Cyber\n    \u251c\u2500\u2500 experiments\n    |   \u251c\u2500\u2500 sagemaker\n    |   \u2502\u00a0\u00a0 \u251c\u2500\u2500 accelerate_config.yaml\n    |   \u2502\u00a0\u00a0 \u251c\u2500\u2500 entry.py\n    |   \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\n    |   \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.ipynb\n    |   \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_script_sagemaker.sh\n    ...\n</code></pre> 3. Set up the folder structure by running the following commands: <pre><code>cp -r Cyber/experiments/sagemaker/* Cyber/\nmv Cyber/run.ipynb .\n</code></pre> After this step your folder structure should look like this: <pre><code>training\n\u251c\u2500\u2500 Cyber\n|   \u251c\u2500\u2500 accelerate_config.yaml\n|   \u251c\u2500\u2500 entry.py\n|   \u251c\u2500\u2500 README.md\n|   \u251c\u2500\u2500 train_script_sagemaker.sh\n|   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 run.ipynb\n    ...\n</code></pre></p> <p>4. Install and authenticate WandB. Run the following commands in the terminal. <pre><code>pip install wandb\nwandb login\n</code></pre></p> <p>5. Open the <code>run.ipynb</code> notebook and follow the instructions to queue the training job. If you are successful, your job should start running on Sagemaker. If you used all default settings, finishing 1 epoch should take around 4 hours, which would set you back around $120. Enjoy! :D</p>"},{"location":"tutorial/action/","title":"CYBER Action Model","text":"<p>In traditional systems, teaching a robot to flip a pancake with a spatula might involve countless hours of training on a specific arm model. But what if you wanted the same task done by a different robot with different sensors or arms?</p> <p>Overcoming Heterogeneity within CYBER allows for cross-embodiment learning\u2014the robot learns fundamental skills that are transferable across different platforms. Picture a scenario where a robot trained in a factory on one set of machinery can seamlessly adapt to a new set of machines at a different facility. CYBER provides human-captured data and teleoperation data to provide a foundation for cross-embodiment learning, avoiding the need to train each robot from scratch for every new task or embodiment.</p> <p>Stay Tuned. </p>"},{"location":"tutorial/control/","title":"CYBER Control Model","text":"<p>In traditional systems, teaching a robot to flip a pancake with a spatula might involve countless hours of training on a specific arm model. But what if you wanted the same task done by a different robot with different sensors or arms? Overcoming Heterogeneity within CYBER allows for cross-embodiment learning\u2014the robot learns fundamental skills that are transferable across different platforms. Picture a scenario where a robot trained in a factory on one set of machinery can seamlessly adapt to a new set of machines at a different facility. CYBER achieves this by leveraging shared data and common skill sets, avoiding the need to train each robot from scratch for every new task or embodiment.</p>"},{"location":"tutorial/memory/","title":"CYBER Memory Model","text":"<p>The development of world models in robotics has long been a cornerstone of advanced research, with most approaches relying heavily on vast, platform-specific datasets. These datasets, while valuable, often limit scalability and generalization to different robotic platforms, restricting their broader applicability.</p> <p>In contrast, CYBER approaches world modeling from a \"first principles\" perspective, drawing inspiration from how humans naturally acquire skills through experience and interaction with their environment. Unlike existing robotic world models, CYBER is the first real-world operational system designed to adapt to diverse and challenging environments. It merges a Physical World Model with a Visual-Language Model (VLM) to create a groundbreaking, holistic framework that enables robots to learn, predict, and perform across various tasks and embodiments.</p> <p>We can also construct a more complex model by combining the world model, action model, perception model, memory model, and control model to build a more complex model for a specific task. Here is an example code snippet:</p> <pre><code># Example usage of CYBER combined models for a complex task\nfrom cyber.models.world import WorldModel\nfrom cyber.models.action import ActionModel\nfrom cyber.models.perception import PerceptionModel\nfrom cyber.models.memory import MemoryModel\nfrom cyber.models.control import ControlModel\n\n# Initialize all models\nworld_model = WorldModel()\naction_model = ActionModel()\nperception_model = PerceptionModel()\nmemory_model = MemoryModel()\ncontrol_model = ControlModel()\n\n# Load pre-trained weights if available\nworld_model.load_weights('path/to/world_model_weights')\naction_model.load_weights('path/to/action_model_weights')\nperception_model.load_weights('path/to/perception_model_weights')\nmemory_model.load_weights('path/to/memory_model_weights')\ncontrol_model.load_weights('path/to/control_model_weights')\n\n# Example input data for the complex task\ninput_data = {\n        'sensor_data': [0.1, 0.2, 0.3],\n        'action_data': [1, 0, 1],\n        'task_specific_data': [0.5, 0.6, 0.7],\n        'additional_context': [1, 1, 0]\n}\n\n# Perceive the environment\nperceived_data = perception_model.process(input_data['sensor_data'])\n\n# Utilize memory for context\ncontextual_data = memory_model.retrieve(input_data['additional_context'])\n\n# Predict the next state using the world model\npredicted_state = world_model.predict({\n        'perceived_data': perceived_data,\n        'contextual_data': contextual_data,\n        'task_specific_data': input_data['task_specific_data']\n})\n\n# Predict the next action using the action model\npredicted_action = action_model.predict({\n        'current_state': predicted_state,\n        'goal_state': [1, 0, 1]\n})\n\n# Control the robot using the control model\ncontrol_signals = control_model.generate(predicted_action)\n\nprint(f\"Predicted State: {predicted_state}\")\nprint(f\"Predicted Action: {predicted_action}\")\nprint(f\"Control Signals: {control_signals}\")\n</code></pre>"},{"location":"tutorial/preception/","title":"Preception","text":"<p>Will be released soon.</p>"},{"location":"tutorial/world/","title":"CYBER World Model","text":"<p>Imagine a robot navigating a cluttered room. For it to operate effectively, it needs to \"see\" beyond raw images\u2014recognizing objects, understanding spatial relationships, and predicting interactions. Physical Encoding achieves this by converting the robot\u2019s visual input into latent codes. These codes represent a distilled understanding of the physical world, akin to how humans quickly grasp the layout of a room upon entering. For example, if the robot sees a chair, the encoded information isn't just \"chair\" but also its position, how it might be moved, and its potential use in future tasks. This high-level encoding enables the robot to focus on important environmental features and make smarter decisions.</p>"},{"location":"tutorial/world/#components-of-a-world-model","title":"Components of a World Model","text":"<p>Cyber world models consists of two main components: an autoencoder and a dynamic model. The autoencoder learns to encode raw observations into a compact representation, while the dynamic model predicts future states in this representation.</p>"},{"location":"tutorial/world/#getting-started","title":"Getting Started","text":"<p>Interfaces for each component are standardized and cross-compatible, allowing you to mix and match different models. This flexibility enables you to experiment with various architectures and training strategies, tailoring the world model to your specific needs. <pre><code>from cyber.models.world import Autoencoder, DynamicModel\nimport cyber.world as world\n\n# Load the Autoencoder\nautoencoder:Autoencoder = world.autoencoder.VQModel.from_pretrained(\"path/to/autoencoder/weights\")\n# Load the Dynamic Model\ndynamic_model:DynamicModel = world.dynamic.STMaskGIT.from_pretrained(\"path/to/dynamic/model/weights\")\n\n# Set the models to evaluation mode for inference\nautoencoder.eval()\ndynamic_model.eval()\n\n# Example input data\ninput_obs = torch.randn(1, 3, 64, 64) # (batch_size, channels, height, width)\n\n# Use the autoencoder to encode the input data\ninput_enc = autoencoder.encode(input_obs)\n\n# Use the dynamic model to predict the next state\npredicted_enc = dynamic_model(encoded_obs)\n\n# Decode back into observation space\npredicted_obs = autoencoder.decode(predicted_obs)\n</code></pre></p>"}]}