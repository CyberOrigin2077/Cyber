def test_train():
    '''
    Test the training loop for genie model
    Uses a mock dataset generated by taking the subset of the pipette dataset
    '''
    from cyber.models.world.dynamic import STMaskGIT
    from cyber.models import CyberModule
    from cyber.dataset import RawTokenDataset
    from omegaconf import OmegaConf

    import torch
    from torch.utils.data import random_split

    genie_conf = OmegaConf.load("experiments/configs/models/world/genie.yaml")
    model: CyberModule = STMaskGIT(genie_conf)  # 35M model
    model.to("cuda")
    model.train()

    window_size = 16
    stride = 15
    batch_size = 4
    lr = 1e-4
    # load the dataset
    pipette_data = RawTokenDataset("tests/fixtures/tokenized_data", 
                                   window_size=window_size, 
                                   stride=stride)

    # split the dataset into training and validation randomly
    train_dataset, _ = random_split(pipette_data, [int(len(pipette_data) * 0.9), len(pipette_data) - int(len(pipette_data) * 0.9)])

    # data loader
    train_collator = model.get_train_collator()
    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_collator, shuffle=True)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    loss_at_steps = []
    for step, batch in enumerate(train_dataloader):
        for k, v in batch.items():
            batch[k] = v.to("cuda")
        loss = model.compute_training_loss(**batch)
        loss_at_steps.append(loss.item())
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if step > 10:
            break
    
    # loss should decrease
    assert loss_at_steps[0] > loss_at_steps[5]
    assert loss_at_steps[5] > loss_at_steps[10]
