def test_train_genie():
    '''
    Test the training loop for genie model
    Uses a mock dataset generated by taking the subset of the pipette dataset
    '''
    from cyber.models.world.dynamic import STMaskGIT
    from cyber.models import CyberModule
    from cyber.dataset import RawTokenDataset
    from omegaconf import OmegaConf

    import torch
    from torch.utils.data import random_split

    genie_conf = OmegaConf.load("experiments/configs/models/world/genie.yaml")
    model: CyberModule = STMaskGIT(genie_conf)  # 35M model
    model.to("cuda")
    model.train()

    window_size = 16
    stride = 15
    batch_size = 4
    lr = 1e-4
    # load the dataset
    pipette_data = RawTokenDataset("tests/fixtures/tokenized_data", 
                                   window_size=window_size, 
                                   stride=stride)

    # split the dataset into training and validation randomly
    train_dataset, _ = random_split(pipette_data, [int(len(pipette_data) * 0.9), len(pipette_data) - int(len(pipette_data) * 0.9)])

    # data loader
    train_collator = model.get_train_collator()
    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_collator, shuffle=True)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    loss_at_steps = []
    for step, batch in enumerate(train_dataloader):
        for k, v in batch.items():
            batch[k] = v.to("cuda")
        loss = model.compute_training_loss(**batch)
        loss_at_steps.append(loss.item())
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if step > 10:
            break
    
    # loss should decrease
    assert loss_at_steps[0] > loss_at_steps[5]
    assert loss_at_steps[5] > loss_at_steps[10]
    torch.cuda.empty_cache()

def test_train_openmagvit2():
    '''
    Test the training loop for openmagvit2 model
    Uses a mock dataset generated by taking the subset of the pipette dataset
    '''
    from cyber.models.world.autoencoder import VQModel
    from cyber.models import CyberModule
    from cyber.dataset import RawImageDataset
    from omegaconf import OmegaConf

    import torch
    from torch.utils.data import random_split
    dtype = torch.bfloat16

    magvit_conf = OmegaConf.load("experiments/configs/models/world/openmagvit2.yaml")
    model: CyberModule = VQModel(magvit_conf)
    model.to("cuda", dtype=dtype)
    model.train()

    
    batch_size = 2
    lr = 1e-4
    # load the dataset
    imagenet_data = RawImageDataset("tests/fixtures/image_data")

    # split the dataset into training and validation randomly
    train_dataset, _ = random_split(imagenet_data, [int(len(imagenet_data) * 0.8), len(imagenet_data) - int(len(imagenet_data) * 0.8)])

    # data loader
    train_collator = model.get_train_collator()
    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_collator, shuffle=True)

    # optimizer GAN style
    opt_gen = torch.optim.Adam(
        list(model.encoder.parameters()) + list(model.decoder.parameters()) + list(model.quantize.parameters()), lr=lr, betas=(0.5, 0.9)
    )
    opt_disc = torch.optim.Adam(model.loss.discriminator.parameters(), lr=lr, betas=(0.5, 0.9))

    gen_loss_at_epoch = []
    gen_loss_steps_for_epoch = []
    for epoch in range(3):
        for step, batch in enumerate(train_dataloader):
            for k, v in batch.items():
                batch[k] = v.to("cuda", dtype=dtype)
            genloss, discloss = model.compute_training_loss(**batch)
            opt_gen.zero_grad()
            genloss.backward()
            opt_gen.step()
            opt_disc.zero_grad()
            discloss.backward()
            opt_disc.step()
            model.global_step += 1
            gen_loss_steps_for_epoch.append(genloss.item())
        gen_loss_at_epoch.append(sum(gen_loss_steps_for_epoch)/len(gen_loss_steps_for_epoch))
    
    # loss should decrease
    assert gen_loss_at_epoch[0] > gen_loss_at_epoch[2]
    torch.cuda.empty_cache()
